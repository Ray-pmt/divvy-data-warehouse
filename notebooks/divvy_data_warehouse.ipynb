{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50f509b-9d15-4e0a-a776-a2eb403bc154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created payments table\nCreated trips table\nCreated riders table\nCreated stations table\n\nVerifying bronze layer tables:\nbronze.payments: 1946607 records\n\nSample data from bronze.payments:\n+----------+------------+------+--------+--------------------+--------------------+\n|payment_id|payment_date|amount|rider_id| ingestion_timestamp|         source_file|\n+----------+------------+------+--------+--------------------+--------------------+\n|    539256|  2020-08-01|  9.00|   21826|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|    539257|  2020-09-01|  9.00|   21826|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|    539258|  2020-10-01|  9.00|   21826|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|    539259|  2020-11-01|  9.00|   21826|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|    539260|  2020-12-01|  9.00|   21826|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n+----------+------------+------+--------+--------------------+--------------------+\n\nbronze.trips: 4584921 records\n\nSample data from bronze.trips:\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+--------------------+--------------------+\n|         trip_id|rideable_type|         start_time|           end_time|start_station_id|end_station_id|rider_id| ingestion_timestamp|         source_file|\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+--------------------+--------------------+\n|7E1E50AC37E2DAD3| classic_bike|2021-08-14 14:01:36|2021-08-14 14:34:49|    TA1309000007|         13089|    2644|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|ADFF32195521E952| classic_bike|2021-08-29 16:16:36|2021-08-29 16:24:43|           13288|  TA1308000031|   37747|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|7C59843DB8D13CC7|electric_bike|2021-08-27 11:06:34|2021-08-27 11:12:52|    TA1307000062|  TA1305000020|   63224|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|5B788004F8A5204C| classic_bike|2021-08-27 07:35:33|2021-08-27 07:59:35|           13353|         13242|   45050|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|078629DD14B634AE| classic_bike|2021-08-08 15:00:30|2021-08-08 15:22:57|           13353|         13242|   33762|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n+----------------+-------------+-------------------+-------------------+----------------+--------------+--------+--------------------+--------------------+\n\nbronze.riders: 75000 records\n\nSample data from bronze.riders:\n+--------+----------+---------+--------------------+----------+------------------+----------------+---------+--------------------+--------------------+\n|rider_id|first_name|last_name|             address|  birthday|account_start_date|account_end_date|is_member| ingestion_timestamp|         source_file|\n+--------+----------+---------+--------------------+----------+------------------+----------------+---------+--------------------+--------------------+\n|    1000|     Diana|    Clark| 1200 Alyssa Squares|1989-02-13|        2019-04-23|            NULL|     true|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|    1001|  Jennifer|    Smith|     397 Diana Ferry|1976-08-10|        2019-11-01|      2020-09-01|     true|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|    1002|     Karen|    Smith|644 Brittany Row ...|1998-08-10|        2022-02-04|            NULL|     true|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|    1003|     Bryan|  Roberts|996 Dickerson Tur...|1999-03-29|        2019-08-26|            NULL|    false|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|    1004|     Jesse|Middleton|7009 Nathan Expre...|1969-04-11|        2019-09-14|            NULL|     true|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n+--------+----------+---------+--------------------+----------+------------------+----------------+---------+--------------------+--------------------+\n\nbronze.stations: 838 records\n\nSample data from bronze.stations:\n+------------+--------------------+-----------------+------------------+--------------------+--------------------+\n|  station_id|                name|         latitude|         longitude| ingestion_timestamp|         source_file|\n+------------+--------------------+-----------------+------------------+--------------------+--------------------+\n|         525|Glenwood Ave & To...|        42.012701|-87.66605799999999|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|KA1503000012|  Clark St & Lake St|41.88579466666667|-87.63110066666668|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|         637|Wood St & Chicago...|        41.895634|        -87.672069|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|       13216|  State St & 33rd St|       41.8347335|       -87.6258275|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n|       18003|Fairbanks St & Su...|41.89580766666667|-87.62025316666669|2024-12-06 04:18:...|dbfs:/FileStore/t...|\n+------------+--------------------+-----------------+------------------+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Create bronze database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "\n",
    "# Create bronze layer tables\n",
    "try:\n",
    "    # Create payments table\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze.payments\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/bronze/payments'\n",
    "    OPTIONS (\n",
    "        header = false\n",
    "    )\n",
    "    AS \n",
    "    SELECT \n",
    "        _c0 as payment_id,\n",
    "        cast(_c1 as date) as payment_date,\n",
    "        cast(_c2 as decimal(10,2)) as amount,\n",
    "        cast(_c3 as int) as rider_id,\n",
    "        current_timestamp() as ingestion_timestamp,\n",
    "        input_file_name() as source_file\n",
    "    FROM csv.`/FileStore/tables/payments.csv`\n",
    "    \"\"\")\n",
    "    print(\"Created payments table\")\n",
    "\n",
    "    # Create trips table\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze.trips\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/bronze/trips'\n",
    "    OPTIONS (\n",
    "        header = false\n",
    "    )\n",
    "    AS \n",
    "    SELECT \n",
    "        _c0 as trip_id,\n",
    "        _c1 as rideable_type,\n",
    "        cast(_c2 as timestamp) as start_time,\n",
    "        cast(_c3 as timestamp) as end_time,\n",
    "        _c4 as start_station_id,\n",
    "        _c5 as end_station_id,\n",
    "        cast(_c6 as int) as rider_id,\n",
    "        current_timestamp() as ingestion_timestamp,\n",
    "        input_file_name() as source_file\n",
    "    FROM csv.`/FileStore/tables/trips.csv`\n",
    "    \"\"\")\n",
    "    print(\"Created trips table\")\n",
    "\n",
    "    # Create riders table\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze.riders\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/bronze/riders'\n",
    "    OPTIONS (\n",
    "        header = false\n",
    "    )\n",
    "    AS \n",
    "    SELECT \n",
    "        cast(_c0 as int) as rider_id,\n",
    "        _c1 as first_name,\n",
    "        _c2 as last_name,\n",
    "        _c3 as address,\n",
    "        cast(_c4 as date) as birthday,\n",
    "        cast(_c5 as date) as account_start_date,\n",
    "        cast(_c6 as date) as account_end_date,\n",
    "        cast(_c7 as boolean) as is_member,\n",
    "        current_timestamp() as ingestion_timestamp,\n",
    "        input_file_name() as source_file\n",
    "    FROM csv.`/FileStore/tables/riders.csv`\n",
    "    \"\"\")\n",
    "    print(\"Created riders table\")\n",
    "\n",
    "    # Create stations table\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze.stations\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/bronze/stations'\n",
    "    OPTIONS (\n",
    "        header = false\n",
    "    )\n",
    "    AS \n",
    "    SELECT \n",
    "        _c0 as station_id,\n",
    "        _c1 as name,\n",
    "        cast(_c2 as double) as latitude,\n",
    "        cast(_c3 as double) as longitude,\n",
    "        current_timestamp() as ingestion_timestamp,\n",
    "        input_file_name() as source_file\n",
    "    FROM csv.`/FileStore/tables/stations.csv`\n",
    "    \"\"\")\n",
    "    print(\"Created stations table\")\n",
    "\n",
    "    # Set Delta Lake properties for all tables\n",
    "    for table in [\"payments\", \"trips\", \"riders\", \"stations\"]:\n",
    "        spark.sql(f\"\"\"\n",
    "        ALTER TABLE bronze.{table}\n",
    "        SET TBLPROPERTIES (\n",
    "            'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "            'delta.autoOptimize.autoCompact' = 'true'\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "    # Verify bronze layer tables\n",
    "    print(\"\\nVerifying bronze layer tables:\")\n",
    "    for table in [\"payments\", \"trips\", \"riders\", \"stations\"]:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as count FROM bronze.{table}\").collect()[0][\"count\"]\n",
    "        print(f\"bronze.{table}: {count} records\")\n",
    "        # Show sample data\n",
    "        print(f\"\\nSample data from bronze.{table}:\")\n",
    "        spark.sql(f\"SELECT * FROM bronze.{table} LIMIT 5\").show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in bronze layer setup: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5590060-7e42-49d1-8f02-a1e899847371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DimDate successfully\nCreated DimTime successfully\n\nVerifying dimension tables:\nbronze.dim_date: 1461 records\n\nSample data from bronze.dim_date:\n+--------+----------+----+-------+-----+---+-----------+----------+------------+----------+----------+\n|date_key| full_date|year|quarter|month|day|day_of_week|month_name|quarter_name|is_weekend|is_holiday|\n+--------+----------+----+-------+-----+---+-----------+----------+------------+----------+----------+\n|20190101|2019-01-01|2019|      1|    1|  1|    Tuesday|   January|          Q1|     false|     false|\n|20190102|2019-01-02|2019|      1|    1|  2|  Wednesday|   January|          Q1|     false|     false|\n|20190103|2019-01-03|2019|      1|    1|  3|   Thursday|   January|          Q1|     false|     false|\n|20190104|2019-01-04|2019|      1|    1|  4|     Friday|   January|          Q1|     false|     false|\n|20190105|2019-01-05|2019|      1|    1|  5|   Saturday|   January|          Q1|      true|     false|\n+--------+----------+----+-------+-----+---+-----------+----------+------------+----------+----------+\n\nbronze.dim_time: 1440 records\n\nSample data from bronze.dim_time:\n+--------+---------+-------+-------+------+-----+-----------+\n|time_key|full_time|hour_24|hour_12|minute|am_pm|time_of_day|\n+--------+---------+-------+-------+------+-----+-----------+\n|       0| 00:00:00|      0|     12|     0|   AM|      Night|\n|       1| 00:01:00|      0|     12|     1|   AM|      Night|\n|       2| 00:02:00|      0|     12|     2|   AM|      Night|\n|       3| 00:03:00|      0|     12|     3|   AM|      Night|\n|       4| 00:04:00|      0|     12|     4|   AM|      Night|\n+--------+---------+-------+-------+------+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create dimension tables for dates and times\n",
    "try:\n",
    "    # Create DimDate\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze.dim_date\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/bronze/dim_date'\n",
    "    AS\n",
    "    WITH date_cte AS (\n",
    "        SELECT explode(sequence(\n",
    "            to_date('2019-01-01'),\n",
    "            to_date('2022-12-31'),\n",
    "            interval 1 day\n",
    "        )) as full_date\n",
    "    )\n",
    "    SELECT\n",
    "        date_format(full_date, 'yyyyMMdd') as date_key,\n",
    "        full_date,\n",
    "        year(full_date) as year,\n",
    "        quarter(full_date) as quarter,\n",
    "        month(full_date) as month,\n",
    "        dayofmonth(full_date) as day,\n",
    "        date_format(full_date, 'EEEE') as day_of_week,\n",
    "        date_format(full_date, 'MMMM') as month_name,\n",
    "        concat('Q', quarter(full_date)) as quarter_name,\n",
    "        dayofweek(full_date) IN (1, 7) as is_weekend,\n",
    "        false as is_holiday\n",
    "    FROM date_cte\n",
    "    \"\"\")\n",
    "    print(\"Created DimDate successfully\")\n",
    "\n",
    "    # Create DimTime\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS bronze.dim_time\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/bronze/dim_time'\n",
    "    AS\n",
    "    WITH time_cte AS (\n",
    "        SELECT explode(sequence(0, 1439)) as minute_of_day\n",
    "    )\n",
    "    SELECT\n",
    "        minute_of_day as time_key,\n",
    "        concat(\n",
    "            lpad(cast(floor(minute_of_day/60) as string), 2, '0'),\n",
    "            ':',\n",
    "            lpad(cast(minute_of_day % 60 as string), 2, '0'),\n",
    "            ':00'\n",
    "        ) as full_time,\n",
    "        floor(minute_of_day/60) as hour_24,\n",
    "        case\n",
    "            when floor(minute_of_day/60) = 0 then 12\n",
    "            when floor(minute_of_day/60) <= 12 then floor(minute_of_day/60)\n",
    "            else floor(minute_of_day/60) - 12\n",
    "        end as hour_12,\n",
    "        minute_of_day % 60 as minute,\n",
    "        case when floor(minute_of_day/60) < 12 then 'AM' else 'PM' end as am_pm,\n",
    "        case\n",
    "            when floor(minute_of_day/60) < 6 then 'Night'\n",
    "            when floor(minute_of_day/60) < 12 then 'Morning'\n",
    "            when floor(minute_of_day/60) < 17 then 'Afternoon'\n",
    "            when floor(minute_of_day/60) < 21 then 'Evening'\n",
    "            else 'Night'\n",
    "        end as time_of_day\n",
    "    FROM time_cte\n",
    "    ORDER BY minute_of_day\n",
    "    \"\"\")\n",
    "    print(\"Created DimTime successfully\")\n",
    "\n",
    "    # Set Delta properties for dimension tables\n",
    "    for table in [\"dim_date\", \"dim_time\"]:\n",
    "        spark.sql(f\"\"\"\n",
    "        ALTER TABLE bronze.{table}\n",
    "        SET TBLPROPERTIES (\n",
    "            'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "            'delta.autoOptimize.autoCompact' = 'true'\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "    # Verify dimension tables\n",
    "    print(\"\\nVerifying dimension tables:\")\n",
    "    for table in [\"dim_date\", \"dim_time\"]:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as count FROM bronze.{table}\").collect()[0][\"count\"]\n",
    "        print(f\"bronze.{table}: {count} records\")\n",
    "        print(f\"\\nSample data from bronze.{table}:\")\n",
    "        spark.sql(f\"SELECT * FROM bronze.{table} LIMIT 5\").show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dimension tables: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f10aaa8-ac38-48cf-89a4-c0fbff86d580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nVerifying gold layer tables:\ngold.dim_station: 838 records\n\nSchema for gold.dim_station:\n+-------------+---------+-------+\n|     col_name|data_type|comment|\n+-------------+---------+-------+\n|  station_key|   string|   NULL|\n|   station_id|   string|   NULL|\n|         name|   string|   NULL|\n|     latitude|   double|   NULL|\n|    longitude|   double|   NULL|\n|city_district|   string|   NULL|\n+-------------+---------+-------+\n\ngold.dim_rider: 75000 records\n\nSchema for gold.dim_rider:\n+--------------------+---------+-------+\n|            col_name|data_type|comment|\n+--------------------+---------+-------+\n|           rider_key|   string|   NULL|\n|            rider_id|      int|   NULL|\n|          first_name|   string|   NULL|\n|           last_name|   string|   NULL|\n|            birthday|     date|   NULL|\n|         current_age|   bigint|   NULL|\n|  account_start_date|     date|   NULL|\n|    account_end_date|     date|   NULL|\n|           is_member|  boolean|   NULL|\n|age_at_account_start|   bigint|   NULL|\n|        rider_status|   string|   NULL|\n+--------------------+---------+-------+\n\ngold.dim_date: 1461 records\n\nSchema for gold.dim_date:\n+------------+---------+-------+\n|    col_name|data_type|comment|\n+------------+---------+-------+\n|    date_key|   string|   NULL|\n|   full_date|     date|   NULL|\n|        year|      int|   NULL|\n|     quarter|      int|   NULL|\n|       month|      int|   NULL|\n|         day|      int|   NULL|\n| day_of_week|   string|   NULL|\n|  month_name|   string|   NULL|\n|quarter_name|   string|   NULL|\n|  is_weekend|  boolean|   NULL|\n|  is_holiday|  boolean|   NULL|\n+------------+---------+-------+\n\ngold.dim_time: 1440 records\n\nSchema for gold.dim_time:\n+-----------+---------+-------+\n|   col_name|data_type|comment|\n+-----------+---------+-------+\n|   time_key|      int|   NULL|\n|  full_time|   string|   NULL|\n|    hour_24|   bigint|   NULL|\n|    hour_12|   bigint|   NULL|\n|     minute|      int|   NULL|\n|      am_pm|   string|   NULL|\n|time_of_day|   string|   NULL|\n+-----------+---------+-------+\n\ngold.fact_trip: 4584921 records\n\nSchema for gold.fact_trip:\n+--------------------+---------+-------+\n|            col_name|data_type|comment|\n+--------------------+---------+-------+\n|            trip_key|   string|   NULL|\n|           rider_key|   string|   NULL|\n|   start_station_key|   string|   NULL|\n|     end_station_key|   string|   NULL|\n|      start_date_key|      int|   NULL|\n|      start_time_key|      int|   NULL|\n|        end_date_key|      int|   NULL|\n|        end_time_key|      int|   NULL|\n|trip_duration_min...|   double|   NULL|\n|       rideable_type|   string|   NULL|\n|# Partition Infor...|         |       |\n|          # col_name|data_type|comment|\n|      start_date_key|      int|   NULL|\n+--------------------+---------+-------+\n\ngold.fact_payment: 1946607 records\n\nSchema for gold.fact_payment:\n+-----------+-------------+-------+\n|   col_name|    data_type|comment|\n+-----------+-------------+-------+\n|payment_key|       string|   NULL|\n|  rider_key|       string|   NULL|\n|   date_key|          int|   NULL|\n|     amount|decimal(10,2)|   NULL|\n+-----------+-------------+-------+\n\n\nFinal database overview:\n\nTables in bronze database:\n+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n|  bronze| dim_date|      false|\n|  bronze| dim_time|      false|\n|  bronze| payments|      false|\n|  bronze|   riders|      false|\n|  bronze| stations|      false|\n|  bronze|    trips|      false|\n+--------+---------+-----------+\n\n\nTables in gold database:\n+--------+------------+-----------+\n|database|   tableName|isTemporary|\n+--------+------------+-----------+\n|    gold|    dim_date|      false|\n|    gold|   dim_rider|      false|\n|    gold| dim_station|      false|\n|    gold|    dim_time|      false|\n|    gold|fact_payment|      false|\n|    gold|   fact_trip|      false|\n+--------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create gold database and implement star schema\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\n",
    "\n",
    "try:\n",
    "    # Create DimStation\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_station\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/gold/dim_station'\n",
    "    AS\n",
    "    SELECT \n",
    "        uuid() as station_key,\n",
    "        station_id,\n",
    "        name,\n",
    "        latitude,\n",
    "        longitude,\n",
    "        CASE \n",
    "            WHEN station_id LIKE 'C%' THEN 'Central District'\n",
    "            WHEN station_id LIKE 'N%' THEN 'North District'\n",
    "            ELSE 'Unknown'\n",
    "        END as city_district\n",
    "    FROM bronze.stations\n",
    "    \"\"\")\n",
    "\n",
    "    # Create DimRider\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_rider\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/gold/dim_rider'\n",
    "    AS\n",
    "    SELECT \n",
    "        uuid() as rider_key,\n",
    "        rider_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        birthday,\n",
    "        floor(months_between(current_date(), birthday)/12) as current_age,\n",
    "        account_start_date,\n",
    "        account_end_date,\n",
    "        is_member,\n",
    "        floor(months_between(account_start_date, birthday)/12) as age_at_account_start,\n",
    "        CASE \n",
    "            WHEN account_end_date IS NULL THEN 'Active'\n",
    "            ELSE 'Inactive'\n",
    "        END as rider_status\n",
    "    FROM bronze.riders\n",
    "    \"\"\")\n",
    "\n",
    "    # Move dimension tables to gold\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_date\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/gold/dim_date'\n",
    "    AS SELECT * FROM bronze.dim_date\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.dim_time\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/gold/dim_time'\n",
    "    AS SELECT * FROM bronze.dim_time\n",
    "    \"\"\")\n",
    "\n",
    "    # Create FactTrip\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.fact_trip\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/gold/fact_trip'\n",
    "    PARTITIONED BY (start_date_key)\n",
    "    AS\n",
    "    SELECT \n",
    "        uuid() as trip_key,\n",
    "        r.rider_key,\n",
    "        ss.station_key as start_station_key,\n",
    "        es.station_key as end_station_key,\n",
    "        cast(date_format(t.start_time, 'yyyyMMdd') as int) as start_date_key,\n",
    "        (hour(t.start_time) * 60 + minute(t.start_time)) as start_time_key,\n",
    "        cast(date_format(t.end_time, 'yyyyMMdd') as int) as end_date_key,\n",
    "        (hour(t.end_time) * 60 + minute(t.end_time)) as end_time_key,\n",
    "        round((unix_timestamp(t.end_time) - unix_timestamp(t.start_time))/60, 2) as trip_duration_minutes,\n",
    "        t.rideable_type\n",
    "    FROM bronze.trips t\n",
    "    LEFT JOIN gold.dim_station ss ON t.start_station_id = ss.station_id\n",
    "    LEFT JOIN gold.dim_station es ON t.end_station_id = es.station_id\n",
    "    LEFT JOIN gold.dim_rider r ON t.rider_id = r.rider_id\n",
    "    \"\"\")\n",
    "\n",
    "    # Create FactPayment\n",
    "    spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS gold.fact_payment\n",
    "    USING DELTA\n",
    "    LOCATION '/FileStore/tables/gold/fact_payment'\n",
    "    AS\n",
    "    SELECT \n",
    "        uuid() as payment_key,\n",
    "        r.rider_key,\n",
    "        cast(date_format(p.payment_date, 'yyyyMMdd') as int) as date_key,\n",
    "        p.amount\n",
    "    FROM bronze.payments p\n",
    "    LEFT JOIN gold.dim_rider r ON p.rider_id = r.rider_id\n",
    "    \"\"\")\n",
    "\n",
    "    # Set Delta properties for gold tables\n",
    "    for table in [\"dim_station\", \"dim_rider\", \"dim_date\", \"dim_time\", \"fact_trip\", \"fact_payment\"]:\n",
    "        spark.sql(f\"\"\"\n",
    "        ALTER TABLE gold.{table}\n",
    "        SET TBLPROPERTIES (\n",
    "            'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "            'delta.autoOptimize.autoCompact' = 'true',\n",
    "            'delta.enableChangeDataFeed' = 'true'\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "    # Optimize fact tables (without partition columns)\n",
    "    spark.sql(\"\"\"\n",
    "    OPTIMIZE gold.fact_trip\n",
    "    ZORDER BY (rider_key, trip_duration_minutes)\n",
    "    \"\"\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "    OPTIMIZE gold.fact_payment\n",
    "    ZORDER BY (rider_key, amount)\n",
    "    \"\"\")\n",
    "\n",
    "    # Verify gold layer tables\n",
    "    print(\"\\nVerifying gold layer tables:\")\n",
    "    for table in [\"dim_station\", \"dim_rider\", \"dim_date\", \"dim_time\", \"fact_trip\", \"fact_payment\"]:\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as count FROM gold.{table}\").collect()[0][\"count\"]\n",
    "        print(f\"gold.{table}: {count} records\")\n",
    "        \n",
    "        # Show schema\n",
    "        print(f\"\\nSchema for gold.{table}:\")\n",
    "        spark.sql(f\"DESCRIBE gold.{table}\").show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in gold layer implementation: {str(e)}\")\n",
    "\n",
    "# Final verification\n",
    "print(\"\\nFinal database overview:\")\n",
    "for database in [\"bronze\", \"gold\"]:\n",
    "    print(f\"\\nTables in {database} database:\")\n",
    "    spark.sql(f\"SHOW TABLES IN {database}\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final NB",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}